import express from 'express';
import cors from 'cors';
import { WebSocketServer } from 'ws';
import { createServer } from 'http';
import { v4 as uuidv4 } from 'uuid';
import { GoogleGenerativeAI } from '@google/generative-ai';
import fetch from 'node-fetch';
import { join, dirname } from 'path';
import { fileURLToPath } from 'url';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const app = express();
const server = createServer(app);
const wss = new WebSocketServer({ server });

app.use(cors());
app.use(express.json());

// Static-Serving f√ºr dist/ (SPA)
app.use(express.static(join(__dirname, 'dist')));
app.get('*', (req, res) => {
  res.sendFile(join(__dirname, 'dist', 'index.html'));
});

// API Keys - In Production √ºber Umgebungsvariablen
const DEEPGRAM_API_KEY = '3e69806feb52b90f01f2e47f9e778fc87b6d811a';
const GEMINI_API_KEY = 'AIzaSyDCqBRhKqrwXGfIbfmQVj3nRbQLDFsGqEI';
const SMALLEST_API_KEY = 'sk-2ad79c9f-cf37-44b3-87dd-0b0b8eb66e5b';

// Initialize Gemini AI
const genAI = new GoogleGenerativeAI(GEMINI_API_KEY);

const clients = new Map();

class VoiceAgent {
  constructor(clientId, ws) {
    this.clientId = clientId;
    this.ws = ws;
    this.deepgramSocket = null;
    this.conversationHistory = [];
    this.currentTranscript = '';
    this.isProcessing = false;
    this.startTime = Date.now();
    this.sampleRate = 48000; // Standard Sample-Rate
  }

  sendToClient(data) {
    if (this.ws && this.ws.readyState === 1) {
      this.ws.send(JSON.stringify({
        ...data,
        timestamp: Date.now(),
        latency: Date.now() - this.startTime
      }));
    }
  }

  async startDeepgramStream() {
    try {
      console.log('üé§ Starte Deepgram Stream f√ºr Client:', this.clientId, 'Sample-Rate:', this.sampleRate);
      
      // Optimierte Deepgram-Konfiguration f√ºr PCM-Streaming und niedrige Latenz
      const deepgramUrl = `wss://api.deepgram.com/v1/listen?` + new URLSearchParams({
        language: 'multi',
        model: 'nova-3',
        punctuate: 'true',
        interim_results: 'true',
        endpointing: '300', // 300ms f√ºr schnelle Erkennung von Satzende
        vad_events: 'true', // Voice Activity Detection
        smart_format: 'true',
        utterance_end_ms: '400', // 400ms f√ºr Utterance End (TTFA <1s)
        encoding: 'linear16', // PCM-Streaming
        sample_rate: this.sampleRate.toString() // Dynamische Sample-Rate vom Client
      });

      const headers = { 
        Authorization: `Token ${DEEPGRAM_API_KEY}`,
        'Content-Type': 'application/json'
      };
      
      const WebSocketDG = (await import('ws')).WebSocket;
      this.deepgramSocket = new WebSocketDG(deepgramUrl, { headers });

      this.deepgramSocket.on('open', () => {
        console.log('‚úÖ Deepgram WebSocket verbunden mit Sample-Rate:', this.sampleRate);
        this.sendToClient({
          type: 'status',
          message: 'Spracherkennung bereit - sprechen Sie jetzt!'
        });
      });

      this.deepgramSocket.on('message', async (msg) => {
        try {
          const data = JSON.parse(msg);
          
          // Speech-to-Text Results
          if (data.channel && data.channel.alternatives && data.channel.alternatives[0]) {
            const transcript = data.channel.alternatives[0].transcript;
            const isFinal = data.is_final;
            const confidence = data.channel.alternatives[0].confidence;
            
            if (transcript && transcript.trim()) {
              this.currentTranscript = transcript;
              
              this.sendToClient({
                type: 'transcript',
                text: transcript,
                isFinal: isFinal,
                confidence: confidence || 0,
                partial: !isFinal
              });

              // Bei finalem Transcript -> Sofort LLM Processing
              if (isFinal && transcript.length > 2 && !this.isProcessing) {
                this.isProcessing = true;
                console.log('ü§ñ Verarbeite:', transcript);
                
                // Parallele Verarbeitung f√ºr noch niedrigere Latenz
                this.processWithGemini(transcript);
              }
            }
          }

          // Voice Activity Detection
          if (data.type === 'UtteranceEnd') {
            console.log('üîö Satzende erkannt');
            this.sendToClient({
              type: 'utterance_end',
              message: 'Satzende erkannt'
            });
          }

        } catch (e) { 
          console.error('‚ùå Deepgram Parse-Fehler:', e); 
        }
      });

      this.deepgramSocket.on('close', (code, reason) => {
        console.log('üîå Deepgram WebSocket geschlossen:', code, reason.toString());
      });

      this.deepgramSocket.on('error', (err) => {
        console.error('‚ùå Deepgram Fehler:', err);
        this.sendToClient({
          type: 'error',
          message: 'Spracherkennung Fehler'
        });
      });

    } catch (error) {
      console.error('‚ùå Deepgram Setup Fehler:', error);
      this.sendToClient({
        type: 'error',
        message: 'Spracherkennung konnte nicht gestartet werden'
      });
    }
  }

  async processWithGemini(transcript) {
    try {
      const processingStart = Date.now();
      
      // Restaurant-spezifischer Prompt f√ºr Buchungen
      const systemPrompt = `Du bist ein freundlicher Telefonassistent f√ºr das Restaurant "Bella Vista". 

Deine Hauptaufgaben:
- Tischreservierungen entgegennehmen 
- Informationen √ºber √ñffnungszeiten geben
- Fragen zur Speisekarte beantworten
- Bei Reservierungen: Name, Datum, Uhrzeit, Personenzahl erfragen

√ñffnungszeiten:
‚Ä¢ Montag-Freitag: 17:00-23:00 Uhr
‚Ä¢ Samstag: 17:00-24:00 Uhr  
‚Ä¢ Sonntag: 17:00-22:00 Uhr

WICHTIG: Antworte SEHR KURZ und nat√ºrlich (max. 20 W√∂rter). Sei freundlich aber effizient.`;

      // Conversation Context f√ºr bessere Antworten
      this.conversationHistory.push(`Kunde: ${transcript}`);
      const context = this.conversationHistory.slice(-8).join('\n'); // Letzte 4 Nachrichten

      const model = genAI.getGenerativeModel({ 
        model: "models/gemini-2.5-flash-lite-preview-0617", // Neuestes & schnellstes Modell!
        generationConfig: {
          temperature: 0.3,
          maxOutputTokens: 50, // Ultra-kurze Antworten f√ºr maximale Speed
          topP: 0.8,
          topK: 40
        }
      });

      const fullPrompt = `${systemPrompt}\n\nAktuelles Gespr√§ch:\n${context}\n\nAssistant:`;

      this.sendToClient({
        type: 'ai_thinking',
        message: 'KI √ºberlegt...'
      });

      // Streaming f√ºr sofortige Antworten
      const result = await model.generateContentStream(fullPrompt);
      let fullResponse = '';
      let firstChunk = true;

      for await (const chunk of result.stream) {
        const chunkText = chunk.text();
        if (chunkText) {
          fullResponse += chunkText;
          
          // Ersten Chunk sofort senden f√ºr gef√ºhlte niedrigere Latenz
          this.sendToClient({
            type: 'llm_chunk',
            text: chunkText,
            isFirst: firstChunk
          });
          firstChunk = false;
        }
      }

      const processingTime = Date.now() - processingStart;
      console.log(`‚ö° Gemini Response in ${processingTime}ms:`, fullResponse.substring(0, 50));

      if (fullResponse.trim()) {
        // Antwort zur History hinzuf√ºgen
        this.conversationHistory.push(`Assistant: ${fullResponse}`);
        
        // Parallel TTS starten (nicht warten)
        this.generateSpeech(fullResponse, processingTime);
      } else {
        this.isProcessing = false;
      }

    } catch (error) {
      console.error('‚ùå Gemini Fehler:', error);
      this.sendToClient({
        type: 'error',
        message: 'KI-Verarbeitung fehlgeschlagen'
      });
      this.isProcessing = false;
    }
  }

  async generateSpeech(text, geminiTime = 0) {
    try {
      const ttsStart = Date.now();
      console.log('üîä Generiere deutsche Sprache:', text.substring(0, 30) + '...');
      
      // Smallest.ai TTS API - Optimiert f√ºr deutsche Sprache
      const response = await fetch('https://api.smallest.ai/v1/audio/speech', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${SMALLEST_API_KEY}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: 'tts-1', // Schnellstes TTS Modell
          input: text,
          voice: 'echo', // Neutrale deutsche Stimme
          response_format: 'mp3',
          speed: 1.1 // Leicht beschleunigt f√ºr Effizienz
        })
      });

      const ttsTime = Date.now() - ttsStart;

      if (response.ok) {
        const audioBuffer = await response.arrayBuffer();
        const base64Audio = Buffer.from(audioBuffer).toString('base64');
        
        console.log(`üéµ TTS in ${ttsTime}ms, Audio: ${(audioBuffer.byteLength / 1024).toFixed(1)}KB`);
        
        this.sendToClient({
          type: 'voice_response',
          transcript: this.currentTranscript,
          response: text,
          audio: base64Audio,
          format: 'mp3',
          metrics: {
            gemini_time: geminiTime,
            tts_time: ttsTime,
            total_time: Date.now() - this.startTime,
            audio_size: audioBuffer.byteLength
          }
        });
      } else {
        const errorText = await response.text();
        console.error('‚ùå TTS Fehler:', response.status, errorText);
        
        // Fallback ohne Audio
        this.sendToClient({
          type: 'voice_response',
          transcript: this.currentTranscript,
          response: text,
          audio: null,
          error: 'TTS fehlgeschlagen'
        });
      }

    } catch (error) {
      console.error('‚ùå TTS Fehler:', error);
      
      // Fallback ohne Audio
      this.sendToClient({
        type: 'voice_response',
        transcript: this.currentTranscript,
        response: text,
        audio: null,
        error: error.message
      });
    } finally {
      this.isProcessing = false;
      this.startTime = Date.now(); // Reset f√ºr n√§chste Interaktion
    }
  }

  async handleAudioChunk(chunk) {
    if (!this.deepgramSocket || this.deepgramSocket.readyState !== 1) {
      return;
    }
    
    try {
      // Sende rohe Audio-Daten direkt an Deepgram
      this.deepgramSocket.send(chunk);
    } catch (error) {
      console.error('‚ùå Audio-Chunk Fehler:', error);
    }
  }

  async endDeepgramStream() {
    if (this.deepgramSocket) {
      // Sende KeepAlive bevor close
      try {
        this.deepgramSocket.send(JSON.stringify({type: 'CloseStream'}));
      } catch (e) {}
      
      this.deepgramSocket.close(1000, 'Stream beendet');
      this.deepgramSocket = null;
      console.log('üõë Deepgram Stream beendet f√ºr Client:', this.clientId);
    }
  }

  resetConversation() {
    this.conversationHistory = [];
    this.currentTranscript = '';
    this.isProcessing = false;
    console.log('üîÑ Conversation reset f√ºr Client:', this.clientId);
  }
}

// WebSocket Connection Handler
wss.on('connection', (ws) => {
  const clientId = uuidv4();
  const agent = new VoiceAgent(clientId, ws);
  clients.set(clientId, agent);
  
  console.log(`üë§ Neuer Client verbunden: ${clientId} (Total: ${clients.size})`);
  
  // Willkommensnachricht
  ws.send(JSON.stringify({
    type: 'connected',
    clientId,
    message: 'Voice Agent bereit! üéôÔ∏è Dr√ºcken Sie auf das Mikrofon und sprechen Sie.',
    version: '2.0.0'
  }));

  ws.on('message', async (data, isBinary) => {
    try {
      if (isBinary) {
        // Raw PCM Audio Data (direkt vom Client)
        await agent.handleAudioChunk(data);
        return;
      }

      const message = JSON.parse(data);
      
      switch (message.type) {
        case 'start_recording':
          console.log('üé§ Start Recording f√ºr Client:', clientId, 'Sample-Rate:', message.sample_rate);
          // Setze Sample-Rate vom Client
          if (message.sample_rate) {
            agent.sampleRate = message.sample_rate;
          }
          agent.resetConversation();
          await agent.startDeepgramStream();
          break;
          
        case 'audio_data':
          // Audio als Base64 String (Legacy)
          if (message.audio) {
            const audioBuffer = Buffer.from(message.audio, 'base64');
            await agent.handleAudioChunk(audioBuffer);
          }
          break;
          
        case 'stop_recording':
          console.log('üõë Stop Recording f√ºr Client:', clientId);
          await agent.endDeepgramStream();
          break;
          
        case 'reset_conversation':
          agent.resetConversation();
          agent.sendToClient({
            type: 'conversation_reset',
            message: 'Gespr√§ch zur√ºckgesetzt'
          });
          break;

        case 'ping':
          agent.sendToClient({ type: 'pong', timestamp: Date.now() });
          break;
          
        default:
          console.log('‚ùì Unbekannter Message-Type:', message.type);
      }
    } catch (error) {
      console.error('‚ùå Message Handler Fehler:', error);
      agent.sendToClient({
        type: 'error',
        message: 'Nachrichtenverarbeitung fehlgeschlagen'
      });
    }
  });

  ws.on('close', (code, reason) => {
    console.log(`üëã Client ${clientId} getrennt: ${code} ${reason} (Verbleibend: ${clients.size - 1})`);
    clients.delete(clientId);
    agent.endDeepgramStream();
  });

  ws.on('error', (error) => {
    console.error(`‚ùå WebSocket Fehler f√ºr Client ${clientId}:`, error);
  });
});

// Health Check Endpoint
app.get('/health', (req, res) => {
  res.json({
    status: 'healthy',
    timestamp: new Date().toISOString(),
    uptime: process.uptime(),
    memory: process.memoryUsage(),
    clients: clients.size,
    services: {
      deepgram: '‚úÖ Speech-to-Text',
      gemini: '‚úÖ AI Chat (2.0-flash-exp)',
      smallest_ai: '‚úÖ Text-to-Speech'
    },
    version: '2.0.0',
    node_version: process.version
  });
});

// API Status Check
app.get('/api/status', async (req, res) => {
  const checks = {
    deepgram: false,
    gemini: false,
    smallest: false,
    latency: {}
  };

  try {
    // Deepgram Test
    const dgStart = Date.now();
    const dgResponse = await fetch('https://api.deepgram.com/v1/projects', {
      headers: { 'Authorization': `Token ${DEEPGRAM_API_KEY}` }
    });
    checks.deepgram = dgResponse.ok;
    checks.latency.deepgram = Date.now() - dgStart;
  } catch (e) {
    checks.latency.deepgram = -1;
  }

  try {
    // Gemini Test
    const geminiStart = Date.now();
    const model = genAI.getGenerativeModel({ model: "gemini-2.0-flash-exp" });
    await model.generateContent("test");
    checks.gemini = true;
    checks.latency.gemini = Date.now() - geminiStart;
  } catch (e) {
    checks.latency.gemini = -1;
  }

  try {
    // Smallest.ai Test
    const smallestStart = Date.now();
    const response = await fetch('https://api.smallest.ai/v1/models', {
      headers: { 'Authorization': `Bearer ${SMALLEST_API_KEY}` }
    });
    checks.smallest = response.ok;
    checks.latency.smallest = Date.now() - smallestStart;
  } catch (e) {
    checks.latency.smallest = -1;
  }

  res.json({
    timestamp: new Date().toISOString(),
    api_status: checks,
    overall_health: checks.deepgram && checks.gemini && checks.smallest
  });
});

// Error Handler
app.use((err, req, res, next) => {
  console.error('üí• Server Fehler:', err);
  res.status(500).json({
    error: 'Internal Server Error',
    message: err.message,
    timestamp: new Date().toISOString()
  });
});

const PORT = process.env.PORT || 8080;
const HOST = '0.0.0.0';

server.listen(PORT, HOST, () => {
  console.log('\nüöÄ VOICE AGENT SERVER GESTARTET üöÄ');
  console.log(`üì° Server: http://${HOST}:${PORT}`);
  console.log(`üîó WebSocket: ws://${HOST}:${PORT}`);
  console.log('\nüîß Services:');
  console.log('   üé§ Deepgram STT - Deutsche Spracherkennung');
  console.log('   ü§ñ Gemini 2.0 Flash - Ultraschnelle KI-Antworten');  
  console.log('   üîä Smallest.ai TTS - Deutsche Sprachsynthese');
  console.log('\n‚ö° Optimiert f√ºr minimale Latenz mit Streaming!');
  console.log('üìä Health Check: http://localhost:' + PORT + '/health');
  console.log('üîç API Status: http://localhost:' + PORT + '/api/status\n');
});

// Graceful Shutdown
process.on('SIGTERM', () => {
  console.log('üì¥ Server herunterfahren...');
  server.close(() => {
    console.log('‚úÖ Server beendet');
    process.exit(0);
  });
});

export default server; 